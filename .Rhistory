get_prior(formula2, data = data)
posterior_samples <- posterior_predict(model, data = test_data, draws = 100)
model <- readRDS("data/model4_fit.rds")
#how to test hypotheses?
demo <- read_csv("data/test-data/demo_test.csv")
uld <- read_csv("data/test-data/LU_test.csv")
token <- read_csv("data/test-data/token_test.csv")
#are there any NA values when visit 1?
demo %>%
filter(Visit == 1) %>%
select(-VinelandReceptive, -VinelandExpressive, -VinelandWritten)%>%
summarise(Any_NA = any(is.na(.)))
#there arent if vineland is omitted, wtf is it even?
demo <- demo %>%
group_by(Child.ID) %>%
mutate(ADOS = if_else(is.na(ADOS) & Visit != 1, first(ADOS), ADOS))%>% #kind of unnecessary since I only care about 1st value due to the model specification
mutate(ADOS1 = first(ADOS))%>%
mutate(Socialization1 = first(Socialization))%>%
mutate(VerbalIQ1 = first(ExpressiveLangRaw))%>%
mutate(Diagnosis = if_else(Diagnosis == "A", "ASD", "TD"))%>%
group_by(Child.ID) %>%
mutate(Child.ID = cur_group_id()) #can do this since the order of the children is the same in each dataframe
uld <- uld %>%
rename(Visit = VISIT) %>%
mutate(Visit = as.integer(gsub("[^0-9]", "", Visit))) %>%
rename(Child.ID = SUBJ) %>%
group_by(Child.ID) %>%
mutate(Child.ID = cur_group_id())
token <- token %>%
rename(Child.ID = SUBJ) %>%
group_by(Child.ID) %>% #can do this since the order of the children is the same in each dataframe
mutate(Child.ID = cur_group_id())%>%
rename(Visit = VISIT) %>%
mutate(Visit = as.integer(gsub("[^0-9]", "", Visit)))%>%
select(-X)
test_data <- inner_join(demo,uld)
test_data <- inner_join(test_data, token)
test_data <- test_data %>%
select(Child.ID, Visit, Diagnosis, Socialization1, ADOS1, CHI_MLU, VerbalIQ1 )
#was I wrong to use the 1st visit variables in the model?
posterior_samples <- posterior_predict(model, data = test_data, draws = 100)
?readRDS
posterior_samples <- posterior_predict(model, data = test_data, draws = 100)
?posterior_predict
posterior_samples <- rstantools::posterior_predict(model, data = test_data, draws = 100)
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 100)
#are there any NA values when visit 1?
demo %>%
filter(Visit == 1) %>%
select(-VinelandReceptive, -VinelandExpressive, -VinelandWritten)%>%
summarise(Any_NA = any(is.na(.)))
#there arent if vineland is omitted, wtf is it even?
demo <- demo %>%
group_by(Child.ID) %>%
mutate(ADOS = if_else(is.na(ADOS) & Visit != 1, first(ADOS), ADOS))%>% #kind of unnecessary since I only care about 1st value due to the model specification
mutate(ADOS1 = first(ADOS))%>%
mutate(Socialization1 = first(Socialization))%>%
mutate(VerbalIQ1 = first(ExpressiveLangRaw))%>%
mutate(Diagnosis = if_else(Diagnosis == "A", "ASD", "TD"))%>%
group_by(Child.ID) %>%
mutate(Child.ID = cur_group_id()) #can do this since the order of the children is the same in each dataframe
uld <- uld %>%
rename(Visit = VISIT) %>%
mutate(Visit = as.integer(gsub("[^0-9]", "", Visit))) %>%
rename(Child.ID = SUBJ) %>%
group_by(Child.ID) %>%
mutate(Child.ID = cur_group_id())
demo <- read_csv("data/test-data/demo_test.csv")
uld <- read_csv("data/test-data/LU_test.csv")
token <- read_csv("data/test-data/token_test.csv")
#are there any NA values when visit 1?
demo %>%
filter(Visit == 1) %>%
select(-VinelandReceptive, -VinelandExpressive, -VinelandWritten)%>%
summarise(Any_NA = any(is.na(.)))
#there arent if vineland is omitted, wtf is it even?
demo <- demo %>%
group_by(Child.ID) %>%
mutate(ADOS = if_else(is.na(ADOS) & Visit != 1, first(ADOS), ADOS))%>% #kind of unnecessary since I only care about 1st value due to the model specification
mutate(ADOS1 = first(ADOS))%>%
mutate(Socialization1 = first(Socialization))%>%
mutate(VerbalIQ1 = first(ExpressiveLangRaw))%>%
mutate(Diagnosis = if_else(Diagnosis == "A", "ASD", "TD"))%>%
group_by(Child.ID) %>%
mutate(Child.ID = cur_group_id()) #can do this since the order of the children is the same in each dataframe
uld <- uld %>%
rename(Visit = VISIT) %>%
mutate(Visit = as.integer(gsub("[^0-9]", "", Visit))) %>%
rename(Child.ID = SUBJ) %>%
group_by(Child.ID) %>%
mutate(Child.ID = cur_group_id())
token <- token %>%
rename(Child.ID = SUBJ) %>%
group_by(Child.ID) %>% #can do this since the order of the children is the same in each dataframe
mutate(Child.ID = cur_group_id())%>%
rename(Visit = VISIT) %>%
mutate(Visit = as.integer(gsub("[^0-9]", "", Visit)))%>%
select(-X)
test_data <- inner_join(demo,uld)
test_data <- inner_join(test_data, token)
test_data <- test_data %>%
select(Child.ID, Visit, Diagnosis, Socialization1, ADOS1, CHI_MLU, VerbalIQ1 )
#was I wrong to use the 1st visit variables in the model?
View(test_data)
model <- readRDS("data/model4_fit.rds")
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 100)
View(test_data)
demo <- read_csv("data/test-data/demo_test.csv")
uld <- read_csv("data/test-data/LU_test.csv")
token <- read_csv("data/test-data/token_test.csv")
#are there any NA values when visit 1?
demo %>%
filter(Visit == 1) %>%
select(-VinelandReceptive, -VinelandExpressive, -VinelandWritten)%>%
summarise(Any_NA = any(is.na(.)))
#there arent if vineland is omitted, wtf is it even?
demo <- demo %>%
group_by(Child.ID) %>%
mutate(ADOS = if_else(is.na(ADOS) & Visit != 1, first(ADOS), ADOS))%>% #kind of unnecessary since I only care about 1st value due to the model specification
mutate(ADOS1 = first(ADOS))%>%
mutate(Socialization1 = first(Socialization))%>%
mutate(verbalIQ1 = first(ExpressiveLangRaw))%>%
mutate(Diagnosis = if_else(Diagnosis == "A", "ASD", "TD"))%>%
group_by(Child.ID) %>%
mutate(Child.ID = cur_group_id()) #can do this since the order of the children is the same in each dataframe
uld <- uld %>%
rename(Visit = VISIT) %>%
mutate(Visit = as.integer(gsub("[^0-9]", "", Visit))) %>%
rename(Child.ID = SUBJ) %>%
group_by(Child.ID) %>%
mutate(Child.ID = cur_group_id())
token <- token %>%
rename(Child.ID = SUBJ) %>%
group_by(Child.ID) %>% #can do this since the order of the children is the same in each dataframe
mutate(Child.ID = cur_group_id())%>%
rename(Visit = VISIT) %>%
mutate(Visit = as.integer(gsub("[^0-9]", "", Visit)))%>%
select(-X)
test_data <- inner_join(demo,uld)
test_data <- inner_join(test_data, token)
test_data <- test_data %>%
select(Child.ID, Visit, Diagnosis, Socialization1, ADOS1, CHI_MLU, verbalIQ1)
#was I wrong to use the 1st visit variables in the model?
model <- readRDS("data/model4_fit.rds")
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 100)
predicted_values
posterior_samples
View(test_data)
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 100, allow_new_levels = T)
posterior_samples
predicted_values <- colMeans(posterior_samples)
predicted_values
?rmse
rmse(model, normalized = F, verbose = T)
performance_rmse(model, normalized = F, verbose = T)
performance::rmse(model, normalized = F, verbose = T)
performance::rmse(posterior_samples, normalized = F, verbose = T)
performance::rmse(model, normalized = F, verbose = T)
View(test_data)
# Load the model from the RDS file
model <- readRDS("data/model4_fit.rds")
# Generate posterior predictive samples
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 100, allow_new_levels = TRUE)
predicted_values <- colMeans(posterior_samples)
# Calculate RMSE on the training data
library(performance)  # Load the performance package
# Assuming you have a data frame with observed values in the 'observed' column
observed_values <- training_data$CHI_MLU
# Load the model from the RDS file
model <- readRDS("data/model4_fit.rds")
# Generate posterior predictive samples
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 100, allow_new_levels = TRUE)
predicted_values <- colMeans(posterior_samples)
# Calculate RMSE on the training data
library(performance)  # Load the performance package
# Assuming you have a data frame with observed values in the 'observed' column
observed_values <- test_data$CHI_MLU
# Calculate RMSE
rmse_value <- rmse(observed = observed_values, predicted = predicted_values, normalized = FALSE, verbose = TRUE)
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
print(paste("RMSE:", rmse_value))
# Load the model from the RDS file
model <- readRDS("data/model4_fit.rds")
# Generate posterior predictive samples
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 100, allow_new_levels = TRUE)
predicted_values <- colMeans(posterior_samples)
# Calculate RMSE on the training data
library(performance)  # Load the performance package
# Assuming you have a data frame with observed values in the 'observed' column
observed_values <- test_data$CHI_MLU
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
print(paste("RMSE:", rmse_value))
# Write your code here
performance::rmse(model, normalized = F, verbose = T)
#Response residuals not available to calculate mean square error. (R)MSE is probably not reliable.
model <- readRDS("data/model4_fit.rds")
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 100, allow_new_levels = T)
predicted_values <- colMeans(posterior_samples)
posterior_samples
predicted_values
head(predicted_values)
performance::rmse(model, normalized = F, verbose = T)
# Load the model from the RDS file
model <- readRDS("data/model4_fit.rds")
# Generate posterior predictive samples
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 100, allow_new_levels = TRUE)
predicted_values <- colMeans(posterior_samples)
# Calculate RMSE on the training data
library(performance)  # Load the performance package
# Assuming you have a data frame with observed values in the 'observed' column
observed_values <- test_data$CHI_MLU
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
print(paste("RMSE:", rmse_value))
# Load the model from the RDS file
model <- readRDS("data/model4_fit.rds")
# Generate posterior predictive samples
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 2000, allow_new_levels = TRUE)
predicted_values <- colMeans(posterior_samples)
# Calculate RMSE on the training data
library(performance)  # Load the performance package
# Assuming you have a data frame with observed values in the 'observed' column
observed_values <- test_data$CHI_MLU
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
print(paste("RMSE:", rmse_value))
#[1] "RMSE: 1.10813614276328" error on average?
head(predicted_values)
head(predicted_values)
head(test_data$CHI_MLU)
rmse_value <- sqrt(mean((observed_values - predicted_values)))
model <- readRDS("data/model4_fit.rds")
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 2000, allow_new_levels = TRUE)
predicted_values <- colMeans(posterior_samples)
head(predicted_values)
head(test_data$CHI_MLU)
observed_values <- test_data$CHI_MLU
rmse_value <- sqrt(mean((observed_values - predicted_values)))
print(paste("RMSE:", rmse_value))
#[1] "RMSE: 1.10850774160828" error on average?
head(predicted_values)
head(test_data$CHI_MLU)
observed_values <- test_data$CHI_MLU
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
print(paste("RMSE:", rmse_value))
mae <- mean(abs(observed_values - predicted_values))
mae
rmse_value <- sqrt(mean((predicted_values - observed_values)^2))
print(paste("RMSE:", rmse_value))
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value, "\n", "MAE:", mae))
print(paste("RMSE:", rmse_value, "MAE:", mae))
print(paste("RMSE:", rmse_value, ", MAE:", mae))
print(paste("RMSE:", rmse_value, "& MAE:", mae))
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
# mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value))
#[1] "RMSE: 1.10850774160828" error on average?
?rmse
performance_rmse(model, normalized = FALSE)
?performance_rmse
?rmse
rmse(model, test_data)
rmse(model, data)
performance_rmse(model, normalized = FALSE)
View(data)
rmse(model, data)
mae(model, data)
performance_rmse(model, normalized = FALSE)
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value, "\n", "MAE:", mae_value))
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value, "\n", "MAE:", mae))
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value, "&", "MAE:", mae))
#ExpressiveLangRaw per child ID (to illustrate the difference in trajectories of learning)
ggplot(data, aes(x = Visit, y = ExpressiveLangRaw, color = Diagnosis)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ x, se = FALSE, aes(group = Child.ID)) +
labs(title = "Verbal IQ Over Time, for each child", x = "Visit", y = "ExpressiveLangRaw") +
theme_minimal()
ggplot(data, aes(x = Visit, y = ExpressiveLangRaw, color = Diagnosis)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ x, se = FALSE, aes(group = Diagnosis)) +
labs(title = "Verbal IQ Over Time, for each child", x = "Visit", y = "ExpressiveLangRaw") +
theme_minimal()
#loses out on a lot of the individual differences between slopes
#there are children with downwards going slopes (value higher on 1st than 6th visit)
data %>%
filter(Visit %in% c(1, 6)) %>%  # Filter for only visits 1 and 6
group_by(Child.ID, Visit, Diagnosis) %>%
summarize(min_ELR = min(ExpressiveLangRaw)) %>%
pivot_wider(names_from = Visit, values_from = min_ELR) %>%
filter(`1` > `6`)
#2 children whose visit 6 ExpressiveLangRaw is lower than their visit 1, both ASD, slope is negative because it appears they did not significantly progress (similar values) -> slope being negative is not particularly meaningful.
data %>%
filter(Visit %in% c(1, 6)) %>%  # Filter for only visits 1 and 6
group_by(Child.ID, Visit, Diagnosis) %>%
summarize(min_MLU = min(CHI_MLU)) %>%
pivot_wider(names_from = Visit, values_from = min_MLU) %>%
filter(`1` > `6`)
# there are 6 children whose visit 6 MLU is lower than their visit 1, all ASD, slope is negative because it appears they did not significantly progress (similar values), as such, the fact that the slope is negative is, for most of them, not the significant part. Probably more ASD children whose MLU remained the same but whose slope is slightly positive. Still, there are two children for whom the values appeared to go down ""significantly"" (ID 23, 1 -> 0.5 & ID 36, 1.25 - 0.754)
ggplot(data, aes(x = Visit, y = ExpressiveLangRaw, color = Diagnosis)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ x, se = FALSE, aes(group = Child.ID)) +
labs(title = "Verbal IQ Over Time, for each child", x = "Visit", y = "ExpressiveLangRaw") +
theme_minimal()
print(paste("RMSE:", rmse_value, "&", "MAE:", mae))
head(predicted_values)
head(observed_values)
bayes_R2(m1)
bayes_R2(m2)
bayes_R2(m3)
bayes_R2(m4)
model <- readRDS("data/model1_fit.rds")
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 2000, allow_new_levels = TRUE)
predicted_values <- colMeans(posterior_samples)
observed_values <- test_data$CHI_MLU
head(predicted_values)
head(observed_values)
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value, "&", "MAE:", mae))
#[1] "RMSE: 1.10850774160828" error on average, mae 0.84 (average magnitude of errors between predicted and observed values)
performance_rmse(model, normalized = FALSE)
#Response residuals not available to calculate mean square error. (R)MSE is probably not reliable.
model <- readRDS("data/model4_fit.rds")
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 2000, allow_new_levels = TRUE)
predicted_values <- colMeans(posterior_samples)
observed_values <- test_data$CHI_MLU
head(predicted_values)
head(observed_values)
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value, "&", "MAE:", mae))
#[1] "RMSE: 1.10850774160828" error on average, mae 0.84 (average magnitude of errors between predicted and observed values)
performance_rmse(model, normalized = FALSE)
#Response residuals not available to calculate mean square error. (R)MSE is probably not reliable.
?posterior_predict
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 2000, allow_new_levels = TRUE, seed = 1)
model <- readRDS("data/model4_fit.rds")
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 2000, allow_new_levels = TRUE, seed = 1)
predicted_values <- colMeans(posterior_samples)
observed_values <- test_data$CHI_MLU
head(predicted_values)
head(observed_values)
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value, "&", "MAE:", mae))
#[1] "RMSE: 1.10850774160828" error on average, mae 0.84 (average magnitude of errors between predicted and observed values)
performance_rmse(model, normalized = FALSE)
#Response residuals not available to calculate mean square error. (R)MSE is probably not reliable.
model <- readRDS("data/model4_fit.rds")
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 2000, allow_new_levels = TRUE, seed = 1)
predicted_values <- colMeans(posterior_samples)
observed_values <- test_data$CHI_MLU
head(predicted_values)
head(observed_values)
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value, "&", "MAE:", mae))
#[1] "RMSE: 1.11041799538681" error on average, mae 0.84 (average magnitude of errors between predicted and observed values)
performance_rmse(model, normalized = FALSE)
#Response residuals not available to calculate mean square error. (R)MSE is probably not reliable.
model <- readRDS("data/model4_fit.rds")
set.seed(1)
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 2000, allow_new_levels = T)
predicted_values <- colMeans(posterior_samples)
observed_values <- test_data$CHI_MLU
head(predicted_values)
head(observed_values)
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value, "&", "MAE:", mae))
#[1] "RMSE: 1.11041799538681" error on average, mae 0.84 (average magnitude of errors between predicted and observed values)
performance_rmse(model, normalized = FALSE)
#Response residuals not available to calculate mean square error. (R)MSE is probably not reliable.
model <- readRDS("data/model4_fit.rds")
set.seed(1)
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 2000, allow_new_levels = T)
predicted_values <- colMeans(posterior_samples)
observed_values <- test_data$CHI_MLU
head(predicted_values)
head(observed_values)
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value, "&", "MAE:", mae))
#[1] "RMSE: 1.10" error on average, mae 0.84 (average magnitude of errors between predicted and observed values)
performance_rmse(model, normalized = FALSE)
#Response residuals not available to calculate mean square error. (R)MSE is probably not reliable.
kfold(
m4,
K = 10,
folds = NULL,
save_fits = FALSE,
cores = getOption("mc.cores", 1)
)
m4k <- kfold(
m4,
K = 10,
folds = NULL,
save_fits = FALSE,
cores = getOption("mc.cores", 1)
)
View(m4)
View(m4k)
m4k
m1k <- kfold(
m1,
K = 10,
folds = NULL,
save_fits = FALSE,
cores = getOption("mc.cores", 12),
file = 'data/model1_kfold'
)
m1k <- kfold(
m1,
K = 10,
folds = NULL,
save_fits = FALSE,
file = 'data/model1_kfold'
)
m2k <- kfold(
m2,
K = 10,
folds = NULL,
save_fits = FALSE,
file = 'data/model2_kfold'
)
m3k <- kfold(
m3,
K = 10,
folds = NULL,
save_fits = FALSE,
file = 'data/model3_kfold'
)
m4k <- kfold(
m4,
K = 10,
folds = NULL,
save_fits = FALSE,
file = 'data/model4_kfold'
)
m1k
m1k
m2k
m3k
m4k
m1k
m2k
m3k
m4k
model <- readRDS("data/model3_fit.rds")
set.seed(1)
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 100, allow_new_levels = T)
predicted_values <- colMeans(posterior_samples)
observed_values <- test_data$CHI_MLU
head(predicted_values)
head(observed_values)
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value, "&", "MAE:", mae))
#[1] "RMSE: 1.11" error on average, mae 0.84 (average magnitude of errors between predicted and observed values)
performance_rmse(model, normalized = FALSE)
#Response residuals not available to calculate mean square error. (R)MSE is probably not reliable.
model <- readRDS("data/model2_fit.rds")
set.seed(1)
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 100, allow_new_levels = T)
predicted_values <- colMeans(posterior_samples)
observed_values <- test_data$CHI_MLU
head(predicted_values)
head(observed_values)
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value, "&", "MAE:", mae))
#[1] "RMSE: 1.11" error on average, mae 0.84 (average magnitude of errors between predicted and observed values)
performance_rmse(model, normalized = FALSE)
#Response residuals not available to calculate mean square error. (R)MSE is probably not reliable.
model <- readRDS("data/model1_fit.rds")
set.seed(1)
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 100, allow_new_levels = T)
predicted_values <- colMeans(posterior_samples)
observed_values <- test_data$CHI_MLU
head(predicted_values)
head(observed_values)
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value, "&", "MAE:", mae))
#[1] "RMSE: 1.11" error on average, mae 0.84 (average magnitude of errors between predicted and observed values)
performance_rmse(model, normalized = FALSE)
#Response residuals not available to calculate mean square error. (R)MSE is probably not reliable.
model <- readRDS("data/model4_fit.rds")
set.seed(1)
posterior_samples <- posterior_predict(model, newdata = test_data, draws = 100, allow_new_levels = T)
predicted_values <- colMeans(posterior_samples)
observed_values <- test_data$CHI_MLU
head(predicted_values)
head(observed_values)
rmse_value <- sqrt(mean((observed_values - predicted_values)^2))
mae <- mean(abs(observed_values - predicted_values))
print(paste("RMSE:", rmse_value, "&", "MAE:", mae))
#[1] "RMSE: 1.11" error on average, mae 0.84 (average magnitude of errors between predicted and observed values)
performance_rmse(model, normalized = FALSE)
#Response residuals not available to calculate mean square error. (R)MSE is probably not reliable.
m3
m4
bayes_R2(m3)
bayes_R2(m4)
loo_model_weights( m1, m2, m3, m4)
loo_compare( m1, m2, m3, m4)
