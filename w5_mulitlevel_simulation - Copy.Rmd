---
title: "Methods 3, Week 5:"
subtitle: "Multilevel simulation"
knit: (function(inputFile, encoding) {
  rmarkdown::render(
    inputFile,
    encoding = encoding,
    output_file = "w5_multilevel_simulation",
    output_dir = 'documents/slides/',
    output_format = 'all')})
output:
  html_document:
    highlight: zenburn
    theme: united
  pdf_document:
    toc: no
    number_sections: no
---

# Exercice with simulated data

Let's imagine we have n autistic and n neurotypical children. We are simulating their average utterance length (Mean Length of Utterance or MLU) in terms of words, starting at Visit 1 and all the way to Visit 6.

- Assumptions
  - Population means are exact values
  - Change by visit is linear (same between each visit)

Remember the usual bayesian workflow:
- define the formula
- define the prior
- prior predictive checks
- fit the model
- model quality checks: traceplots, divergences, rhat, effective samples
- model quality checks: posterior predictive checks, prior-posterior update checks
- model comparison


```{r install_packages}

knitr::opts_chunk$set(echo = TRUE)

if(!'cmdstanr' %in% installed.packages()){
  remotes::install_github("stan-dev/cmdstanr")
  cmdstanr::install_cmdstan()}


pacman::p_load(
  tidyverse,
  brms,
  patchwork
)

d <- read.csv("data/data_clean.csv")
```

```{r data_configuration}

# Define the sample size per group
n <- 50
visits <- 6

# Define the dataset (1000 td, 1000 asd, 6 visits)
sim_data <- tibble(expand.grid(child_id = seq(n), diagnosis = c("asd", "td"), visit = seq(visits)), intercept = NA, slope = NA)
# Make sure ASD and TD do not share a ID
sim_data$child_id[sim_data$diagnosis == "td"] <-
  sim_data$child_id[sim_data$diagnosis == "td"] + n

sim_data
```

```{r parameter_value_setting}

# Define the parameters, based on literature and assumptions
params <- list(
  mu_asd = 2,
  sigma_asd = 0.4,
  mu_td = 2,
  sigma_td = 0.3,
  mu_visit_asd = 0.4,
  sigma_visit_asd = 0.3,
  mu_visit_td = 0.6,
  sigma_visit_td = 0.2,
  error = 0.2)

```

```{r simulate_data}


#define the intercept and slope

for (i in seq(unique(sim_data$child_id))) {
  sim_data$intercept[sim_data$child_id == i &
                       sim_data$diagnosis == "asd"] <-
    rnorm(1, params$mu_asd, params$sigma_asd)
  sim_data$intercept[sim_data$child_id == i &
                       sim_data$diagnosis == "td"] <-
    rnorm(1, params$mu_td, params$sigma_td)
  sim_data$slope[sim_data$child_id == i &
                   sim_data$diagnosis == "asd"] <-
    rnorm(1, params$mu_visit_asd, params$sigma_visit_asd)
  sim_data$slope[sim_data$child_id == i &
                   sim_data$diagnosis == "td"] <-
    rnorm(1, params$mu_visit_td, params$sigma_visit_td)  
}

# Calculate mlu per each data point
sim_data <- sim_data %>%
  mutate(
    mlu = intercept +
      (slope * (visit - 1)) + rnorm(1, 0, params$error))
```

# Explore Simulated Data

```{r view_simulated_data}
# Check the data - looking at the first few rows
sim_data
```

# Plot Simulated Data

```{r plot_simulated_data}

# Check the data - plotting the data
ggplot(sim_data, aes(visit, mlu, color = diagnosis)) +
  geom_point(alpha = 0.2) +
  geom_smooth(method = lm)

```

# Formulas

```{r syntax}

# The left-hand side defines the outcome (what is predicted)
# The right-hand side defines the predictors
# Constant effects are added first, followed by varying ones
# Varying effects are specified between brackets. Again the value 1 is to indicate the intercept and the variables right of the vertical “|” bar is used to indicate grouping variables.
# All correlations between each pair of random effects are estimated
# Use || between two random effects instead of | to prevent this.

formula1 <-
  brms::bf(CHI_MLU ~ Visit + (1|Visit) + (1 + Visit|Child.ID))

# Intercepts are assumed by default in R.
# Hence, the above formula is equivalent to the following one:
# mlu ~ 1 + visit + (1 + visit |child_id)
# To fit a model without a population-level intercept, replace the 1 in the previous formula by a 0
# Cross level interactions between two predictors can be created by the : sign or by multiplying them instead of simply adding them

formula2 <-
  brms::bf(CHI_MLU ~ 0 +
             Diagnosis*Visit +
             (1 + Visit | gr(Child.ID, by = Diagnosis)))


formula3 <- brms::bf(CHI_MLU ~ 0 +
                       Diagnosis +
                       Diagnosis:Visit +
                       (1 + Visit | gr(Child.ID, by = Diagnosis)),
                     sigma ~ 0 +
                       Diagnosis +
                       (1 | gr(Child.ID, by = Diagnosis)))

```

# Priors

```{r manage_priors}
sim_data <- sim_data %>%
  rename(Child.ID= child_id, CHI_MLU = mlu, Diagnosis = diagnosis, Visit = visit)
```


```{r manage_priors}
# Get priors
get_prior(formula1, data = sim_data)

priors1 <- c(
  prior(normal(1, 1), class = Intercept),
  prior(normal(0, .5), class = b),
  prior(normal(0, 1), class = sd),
  prior(normal(0, 1), class = sigma),
  prior(lkj(3), class = cor) #why lkj3 when its not in the output?
)

get_prior(formula2, data = sim_data)

priors2 <- c(
  prior(normal(0, .5), class = b),
  prior(normal(1, 1), class = b, coef = Diagnosisasd),
  prior(normal(1, 1), class = b, coef = Diagnosistd),
  prior(normal(0, 1), class = sd),
  prior(normal(0, 1), class = sigma),
  prior(lkj(3), class = cor)
)

get_prior(formula3, data = sim_data)

priors3 <- c(
  prior(normal(1, 1), class = b),
  prior(normal(1, 1), class = b, coef = Diagnosisasd),
  prior(normal(1, 1), class = b, coef = Diagnosistd),
  prior(normal(0, 1), class = sd),
  prior(normal(0, 1), class = b, dpar = sigma), #
  prior(normal(0, .5), class = b, dpar = sigma),
  prior(normal(0, 1), class = sd, dpar = sigma), #
  prior(normal(0, .1), class = sd, dpar = sigma),
  prior(lkj(3), class = cor)
)
```

```{r check_prior_predictive}

# Generate prior predictive models

model1_prior <- brm(
  formula1,
  sim_data,
  family = gaussian,
  prior = priors1,
  sample_prior = "only",
  file = 'data/model1_prior',
  backend = "cmdstanr",
  chains = 2,
  stan_model_args = list(stanc_options = list("O1"))
)

model2_prior <- brm(
  formula2,
  sim_data,
  family = gaussian,
  prior = priors2,
  sample_prior = "only",
  file = 'data/model2_prior',
  backend = "cmdstanr",
  chains = 2,
  stan_model_args = list(stanc_options = list("O1"))
)

model3_prior <- brm(
  formula3,
  sim_data,
  family = gaussian,
  prior = priors3,
  sample_prior = "only",
  file = 'data/model3_prior',
  backend = "cmdstanr",
  chains = 2,
  stan_model_args = list(stanc_options = list("O1"))
)

# Check prior predictive checks
prior_predictive1 <- pp_check(model1_prior, ndraws = 100)
prior_predictive2 <- pp_check(model2_prior, ndraws = 100)
prior_predictive3 <- pp_check(model3_prior, ndraws = 100)

prior_predictive1 + prior_predictive2 + prior_predictive3
```

##past the point
#what is m_p0? the prior
```{r fit_model}

prior1model <- readRDS('data/model1_prior.rds')
prior2model <- readRDS('data/model2_prior.rds')
prior3model <- readRDS('data/model3_prior.rds')

#
model1 <- brm(
  formula1,
  d,
  family = gaussian,
  prior = priors1,
  sample_prior = T,
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(
    adapt_delta = 0.9,
    max_treedepth = 20
  ),
  stan_model_args = list(stanc_options = list("O1"))
)

model2 <- brm(
  formula2,
  d,
  family = gaussian,
  prior = priors2,
  sample_prior = T,
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(
    adapt_delta = 0.9,
    max_treedepth = 20
  ),
  stan_model_args = list(stanc_options = list("O1"))
)

model3 <- brm(
  formula3,
  d,
  family = gaussian,
  prior = priors3,
  sample_prior = T,
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(
    adapt_delta = 0.9,
    max_treedepth = 20
  ),
  stan_model_args = list(stanc_options = list("O1"))
)

# basic model quality checks

```

```{r check_posterior_predictives}
# Posterior predictive checks
p3 <- pp_check(model1, ndraws = 100)
p4 <- pp_check(model2, ndraws = 100)
p5 <- pp_check(model3, ndraws = 100)

(p0 + p1 + p2) / (p3 + p4 + p5)

# Extracting posterior samples
post0 <- as_draws_df(m0)
post1 <- as_draws_df(m1)
post2 <- as_draws_df(m2)

ggplot(post0) +
  geom_histogram(aes(Intercept), alpha = 0.5, fill = "green") +
  geom_histogram(aes(prior_Intercept), alpha = 0.5, fill = "red") +
  theme_bw()

ggplot(post0) +
  geom_histogram(aes(b_Visit), alpha = 0.5, fill = "green") +
  geom_histogram(aes(prior_b), alpha = 0.5, fill = "red") +
  theme_bw()

ggplot(post0) +
  geom_histogram(aes(sd_ID__Intercept), alpha = 0.5, fill = "green") +
  geom_histogram(aes(sd_ID__Visit), alpha = 0.5, fill = "green") +
  geom_histogram(aes(prior_sd_ID), alpha = 0.5, fill = "red") +
  theme_bw()

ggplot(post0) +
  geom_histogram(aes(sigma), alpha = 0.5, fill = "green") +
  geom_histogram(aes(prior_sigma), alpha = 0.5, fill = "red") +
  theme_bw()

ggplot(post1) +
  geom_histogram(aes(b_DiagnosisASD), alpha = 0.5, fill = "green") +
  geom_histogram(aes(b_DiagnosisTD), alpha = 0.5, fill = "blue") +
  geom_histogram(aes(prior_b_DiagnosisASD), alpha = 0.5, fill = "red") +
  theme_bw()

post1 %>% mutate(
  Effect = b_DiagnosisTD - b_DiagnosisASD,
  PriorEffect = prior_b_DiagnosisTD - prior_b_DiagnosisASD
) %>% ggplot() +
  geom_histogram(aes(Effect), alpha = 0.5, fill = "green") +
  geom_histogram(aes(PriorEffect), alpha = 0.5, fill = "red") +
  theme_bw()

ggplot(post1) +
  geom_histogram(aes(`b_DiagnosisASD:Visit`), alpha = 0.5, fill = "green") +
  geom_histogram(aes(`b_DiagnosisTD:Visit`), alpha = 0.5, fill = "blue") +
  geom_histogram(aes(`prior_b_DiagnosisASD:Visit`), alpha = 0.5, fill = "red") +
  theme_bw()

ggplot(post1) +
  geom_histogram(aes(`cor_ID__Intercept:DiagnosisASD__Visit:DiagnosisASD`), alpha = 0.5, fill = "green") +
  geom_histogram(aes(`cor_ID__Intercept:DiagnosisTD__Visit:DiagnosisTD`), alpha = 0.5, fill = "blue") +
  geom_histogram(aes(prior_cor_ID__1), alpha = 0.5, fill = "red")


ggplot(post1) +
  geom_histogram(aes(`sd_ID__Intercept:DiagnosisASD`), alpha = 0.5, fill = "darkgreen") +
  geom_histogram(aes(`sd_ID__Intercept:DiagnosisTD`), alpha = 0.5, fill = "lightgreen") +
  geom_histogram(aes(`sd_ID__Visit:DiagnosisASD`), alpha = 0.5, fill = "darkblue") +
  geom_histogram(aes(`sd_ID__Visit:DiagnosisTD`), alpha = 0.5, fill = "lightblue") +
  geom_histogram(aes(prior_sd_ID), alpha = 0.5, fill = "red") +
  theme_bw()

ggplot(post1) +
  geom_histogram(aes(sigma), alpha = 0.5, fill = "green") +
  geom_histogram(aes(prior_sigma), alpha = 0.5, fill = "red") +
  theme_bw()
```


```{r compare_models}
# Model comparison
m0 <- add_criterion(m0, criterion = "loo")
m1 <- add_criterion(m1, criterion = "loo")
m2 <- add_criterion(m2, criterion = "loo")

loo_compare(m0, m1, m2)
loo_model_weights(m0, m1, m2)
```

## Now let's loop

# Create a function to simulate the data given n and seed

```{r loop}
n_sim <- 10

sim_d_and_fit <- function(seed, n, visits) {

  set.seed(seed)
  print(n)
  print(seed)

  visits <- 6
  mu_asd <- 1.5
  sigma_asd <- 0.5
  mu_td <- 1.5
  sigma_td <- 0.3
  mu_visit_asd <- 0.4
  sigma_visit_asd <- 0.4
  mu_visit_td <- 0.6
  sigma_visit_td <- 0.2
  error <- 0.2

  d <- tibble(expand.grid(ID = seq(n), Diagnosis = c("ASD", "TD"), Visit = seq(visits)))

  d <- d %>% mutate(
    IndividualIntercept = NA,
    IndividualSlope = NA
  )

  for (i in seq(n)) {
    d$IndividualIntercept[d$ID == i & d$Diagnosis == "ASD"] <- rnorm(1, mu_asd, sigma_asd)
    d$IndividualIntercept[d$ID == i & d$Diagnosis == "TD"] <- rnorm(1, mu_td, sigma_td)
    d$IndividualSlope[d$ID == i & d$Diagnosis == "ASD"] <- rnorm(1, mu_visit_asd, sigma_visit_asd)
    d$IndividualSlope[d$ID == i & d$Diagnosis == "TD"] <- rnorm(1, mu_visit_td, sigma_visit_td)
  }

  # Calculate MLU per each datapoint
  d <- d %>% mutate(
    MLU = IndividualIntercept + IndividualSlope * (Visit - 1) + rnorm(1, 0, error),
    MLU = ifelse(MLU < 0, 0, MLU) # MLUs below 0 are impossible
  )

  d$ID[d$Diagnosis == "TD"] <- d$ID[d$Diagnosis == "TD"] + 1000

  m1a <- update(m1,
         newdata = d,
         backend = "cmdstanr",
         chains = 2,
         cores = 2,
         threads = threading(2),
         seed = seed,
         control = list(
           adapt_delta = 0.9,
           max_treedepth = 20
         ),
         stan_model_args = list(stanc_options = list("O1")) )

  estimate <- m1a %>%  ## FROM HERE!
    as_draws_df() %>%
    mutate(Effect = `b_DiagnosisTD:Visit` - `b_DiagnosisASD:Visit`) %>%
    summarize(
      EffectMean = mean(Effect),
      EffectSE = sd(Effect),
      EffectLowCI = quantile(Effect, 0.025),
      EffectHighCI = quantile(Effect, 0.975),
      EffectWidth = EffectHighCI - EffectLowCI,
      ER = sum(Effect > 0) / sum(Effect <= 0),
      Cred = sum(Effect > 0) / n()
    )
    return(estimate)
}

## Simulate, fit the model, extract the relevant estimates, take the time
t5 <- Sys.time()
s3 <-
  tibble(seed = 1000:1009) %>% # Actually best to stick to 1000-1009 (seeds 3 and 1010 take forever to fit)
  mutate(b1 = map(seed, sim_d_and_fit, n, visits)) %>%
  unnest(b1)
t6 <- Sys.time()
t6 - t5

## Plot results

s3 %>%
  ggplot(aes(x = seed, y = EffectMean, ymin = EffectLowCI, ymax = EffectHighCI)) +
  geom_hline(yintercept = c(0, .5), color = "white") +
  geom_pointrange(fatten = 1/2) +
  labs(x = "seed (i.e., simulation index)",
       y = expression(beta[1])) +
  theme_bw()

s3 %>%
  mutate(check = ifelse(EffectLowCI > 0, 1, 0)) %>%
  summarise(power = mean(check))

s3 %>%
  mutate(check = ifelse(EffectWidth < .1, 1, 0)) %>%
  summarise(`width power` = mean(check))

s3 %>%
  ggplot(aes(x = reorder(seed, EffectLowCI), y = EffectMean, ymin = EffectLowCI, ymax = EffectHighCI)) +
  geom_hline(yintercept = c(0, .5), color = "white") +
  geom_pointrange(fatten = 1/2) +
  scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) +
  ylab(expression(beta[1])) +
  #coord_cartesian(ylim = c(-.5, 1.3)) +
  theme_bw()
```

```{r assess_power}

## THEN SETUP A LOOP INCREASING N AND ASSESS POWER
d_sim <- NULL
for (n in c(10, 15, 20, 25, 30, 50, 100, 300, 1000)) { #
  temp <-  tibble(seed = 1000:1009) %>%
    mutate(b1 = map(seed, sim_d_and_fit, n)) %>%
    unnest(b1) %>% mutate(sample = n)
  if (exists("d_sim")) {d_sim <- rbind(d_sim, temp)} else {d_sim <- temp}
  write_csv(d_sim, "~/Dropbox/Teaching/2022 - Methods 3/Assignments22/A1_LanguageDevelopment/data/sim_data.csv")
}

write_csv(d_sim, "~/Dropbox/Teaching/2022 - Methods 3/Assignments22/A1_LanguageDevelopment/data/sim_data_10_15_20_25_300_1000.csv")

d_sim  %>%
  ggplot(aes(x = seed, y = EffectMean, ymin = EffectLowCI, ymax = EffectHighCI)) + # x = reorder(seed, EffectLowCI)
  geom_hline(yintercept = .2, color = "red") +
  geom_pointrange(fatten = 1/2) +
  scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) +
  ylab("Estimate") +
  facet_wrap(sample ~ .) +
  #coord_cartesian(ylim = c(-.5, 1.3)) +
  theme_bw()

```

# BONUS question

What if the difference between ASD and TD was 0? how big of a sample size would you need? What about different effect sizes, and different error terms?